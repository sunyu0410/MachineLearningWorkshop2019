{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ml_day3_part_4.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"KtSAO18qrXje","colab_type":"text"},"source":["# Generative adversarial models (2 hours)\n","ACPSEM Machine Learning Workshop 2019, 29 - 31 May 2019\n","\n","Yu Sun, yu.sun@sydney.edu.au\n","\n","University of Sydney\n","\n","In this session, we will look at a generative adversarial network (GAN) using the MNIST data set. There are many subtypes of GAN models. This is an auxiliary classifier GAN (AC-GAN). The original paper can be found [here](https://arxiv.org/abs/1610.09585) and a `keras` implementation [here](https://github.com/eriklindernoren/Keras-GAN/blob/master/acgan/acgan.py).\n","\n","Generative models are typically harder to train. The session below will go through the structure of the code. Pay attention to the different components for the major players in the GAN model, i.e. the generator and the discriminator."]},{"cell_type":"markdown","metadata":{"id":"2O5a5r6d2_i2","colab_type":"text"},"source":["## Code\n","The following example uses the code from here:\n","\n","https://github.com/eriklindernoren/Keras-GAN/blob/master/acgan/acgan.py"]},{"cell_type":"code","metadata":{"id":"ctlzMNaf2tAV","colab_type":"code","colab":{}},"source":["# From https://github.com/eriklindernoren/Keras-GAN/blob/master/acgan/acgan.py\n","\n","from __future__ import print_function, division\n","from keras.datasets import mnist\n","from keras.layers import Input, Dense, Reshape, Flatten, Dropout, multiply\n","from keras.layers import BatchNormalization, Activation, Embedding, ZeroPadding2D\n","from keras.layers.advanced_activations import LeakyReLU\n","from keras.layers.convolutional import UpSampling2D, Conv2D\n","from keras.models import Sequential, Model\n","from keras.optimizers import Adam\n","\n","import matplotlib.pyplot as plt\n","\n","import numpy as np\n","\n","\n","class ACGAN():\n","    def __init__(self):\n","        # Input shape\n","        self.img_rows = 28\n","        self.img_cols = 28\n","        self.channels = 1\n","        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n","        self.num_classes = 10\n","        self.latent_dim = 100\n","\n","        optimizer = Adam(0.0002, 0.5)\n","        losses = ['binary_crossentropy', 'sparse_categorical_crossentropy']\n","\n","        # Build and compile the discriminator\n","        self.discriminator = self.build_discriminator()\n","        self.discriminator.compile(loss=losses,\n","            optimizer=optimizer,\n","            metrics=['accuracy'])\n","\n","        # Build the generator\n","        self.generator = self.build_generator()\n","\n","        # The generator takes noise and the target label as input\n","        # and generates the corresponding digit of that label\n","        noise = Input(shape=(self.latent_dim,))\n","        label = Input(shape=(1,))\n","        img = self.generator([noise, label])\n","\n","        # For the combined model we will only train the generator\n","        self.discriminator.trainable = False\n","\n","        # The discriminator takes generated image as input and determines validity\n","        # and the label of that image\n","        valid, target_label = self.discriminator(img)\n","\n","        # The combined model  (stacked generator and discriminator)\n","        # Trains the generator to fool the discriminator\n","        self.combined = Model([noise, label], [valid, target_label])\n","        self.combined.compile(loss=losses,\n","            optimizer=optimizer)\n","\n","    def build_generator(self):\n","\n","        model = Sequential()\n","\n","        model.add(Dense(128 * 7 * 7, activation=\"relu\", input_dim=self.latent_dim))\n","        model.add(Reshape((7, 7, 128)))\n","        model.add(BatchNormalization(momentum=0.8))\n","        model.add(UpSampling2D())\n","        model.add(Conv2D(128, kernel_size=3, padding=\"same\"))\n","        model.add(Activation(\"relu\"))\n","        model.add(BatchNormalization(momentum=0.8))\n","        model.add(UpSampling2D())\n","        model.add(Conv2D(64, kernel_size=3, padding=\"same\"))\n","        model.add(Activation(\"relu\"))\n","        model.add(BatchNormalization(momentum=0.8))\n","        model.add(Conv2D(self.channels, kernel_size=3, padding='same'))\n","        model.add(Activation(\"tanh\"))\n","\n","        model.summary()\n","\n","        noise = Input(shape=(self.latent_dim,))\n","        label = Input(shape=(1,), dtype='int32')\n","        label_embedding = Flatten()(Embedding(self.num_classes, 100)(label))\n","\n","        model_input = multiply([noise, label_embedding])\n","        img = model(model_input)\n","\n","        return Model([noise, label], img)\n","\n","    def build_discriminator(self):\n","\n","        model = Sequential()\n","\n","        model.add(Conv2D(16, kernel_size=3, strides=2, input_shape=self.img_shape, padding=\"same\"))\n","        model.add(LeakyReLU(alpha=0.2))\n","        model.add(Dropout(0.25))\n","        model.add(Conv2D(32, kernel_size=3, strides=2, padding=\"same\"))\n","        model.add(ZeroPadding2D(padding=((0,1),(0,1))))\n","        model.add(LeakyReLU(alpha=0.2))\n","        model.add(Dropout(0.25))\n","        model.add(BatchNormalization(momentum=0.8))\n","        model.add(Conv2D(64, kernel_size=3, strides=2, padding=\"same\"))\n","        model.add(LeakyReLU(alpha=0.2))\n","        model.add(Dropout(0.25))\n","        model.add(BatchNormalization(momentum=0.8))\n","        model.add(Conv2D(128, kernel_size=3, strides=1, padding=\"same\"))\n","        model.add(LeakyReLU(alpha=0.2))\n","        model.add(Dropout(0.25))\n","\n","        model.add(Flatten())\n","        model.summary()\n","\n","        img = Input(shape=self.img_shape)\n","\n","        # Extract feature representation\n","        features = model(img)\n","\n","        # Determine validity and label of the image\n","        validity = Dense(1, activation=\"sigmoid\")(features)\n","        label = Dense(self.num_classes, activation=\"softmax\")(features)\n","\n","        return Model(img, [validity, label])\n","\n","    def train(self, epochs, batch_size=128, sample_interval=50):\n","\n","        # Load the dataset\n","        (X_train, y_train), (_, _) = mnist.load_data()\n","\n","        # Configure inputs\n","        X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n","        X_train = np.expand_dims(X_train, axis=3)\n","        y_train = y_train.reshape(-1, 1)\n","\n","        # Adversarial ground truths\n","        valid = np.ones((batch_size, 1))\n","        fake = np.zeros((batch_size, 1))\n","\n","        for epoch in range(epochs):\n","\n","            # ---------------------\n","            #  Train Discriminator\n","            # ---------------------\n","\n","            # Select a random batch of images\n","            idx = np.random.randint(0, X_train.shape[0], batch_size)\n","            imgs = X_train[idx]\n","\n","            # Sample noise as generator input\n","            noise = np.random.normal(0, 1, (batch_size, 100))\n","\n","            # The labels of the digits that the generator tries to create an\n","            # image representation of\n","            sampled_labels = np.random.randint(0, 10, (batch_size, 1))\n","\n","            # Generate a half batch of new images\n","            gen_imgs = self.generator.predict([noise, sampled_labels])\n","\n","            # Image labels. 0-9 \n","            img_labels = y_train[idx]\n","\n","            # Train the discriminator\n","            d_loss_real = self.discriminator.train_on_batch(imgs, [valid, img_labels])\n","            d_loss_fake = self.discriminator.train_on_batch(gen_imgs, [fake, sampled_labels])\n","            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n","\n","            # ---------------------\n","            #  Train Generator\n","            # ---------------------\n","\n","            # Train the generator\n","            g_loss = self.combined.train_on_batch([noise, sampled_labels], [valid, sampled_labels])\n","\n","            # Plot the progress\n","            print (\"%d [D loss: %f, acc.: %.2f%%, op_acc: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[3], 100*d_loss[4], g_loss[0]))\n","\n","            # If at save interval => save generated image samples\n","            if epoch % sample_interval == 0:\n","                self.save_model()\n","                self.sample_images(epoch)\n","\n","    def sample_images(self, epoch):\n","        r, c = 10, 10\n","        noise = np.random.normal(0, 1, (r * c, 100))\n","        sampled_labels = np.array([num for _ in range(r) for num in range(c)])\n","        gen_imgs = self.generator.predict([noise, sampled_labels])\n","        # Rescale images 0 - 1\n","        gen_imgs = 0.5 * gen_imgs + 0.5\n","\n","        fig, axs = plt.subplots(r, c)\n","        cnt = 0\n","        for i in range(r):\n","            for j in range(c):\n","                axs[i,j].imshow(gen_imgs[cnt,:,:,0], cmap='gray')\n","                axs[i,j].axis('off')\n","                cnt += 1\n","        fig.savefig(\"images/%d.png\" % epoch)\n","        plt.close()\n","\n","    def save_model(self):\n","\n","        def save(model, model_name):\n","            model_path = \"saved_model/%s.json\" % model_name\n","            weights_path = \"saved_model/%s_weights.hdf5\" % model_name\n","            options = {\"file_arch\": model_path,\n","                        \"file_weight\": weights_path}\n","            json_string = model.to_json()\n","            open(options['file_arch'], 'w').write(json_string)\n","            model.save_weights(options['file_weight'])\n","\n","        save(self.generator, \"generator\")\n","        save(self.discriminator, \"discriminator\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"whpZc25D22ZV","colab_type":"code","colab":{}},"source":["if __name__ == '__main__':\n","    acgan = ACGAN()\n","    acgan.train(epochs=14000, batch_size=32, sample_interval=200)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zZ0Y2YWYfm4C","colab_type":"text"},"source":["## Results\n","The output trained by myself (Yu Sun) is uploaded as the `acgan.zip`. Upload the file to the server, unzip it and use `keras.models.model_from_json` to load the models in the `saved_models` folder. "]},{"cell_type":"markdown","metadata":{"id":"D0yg41P627_-","colab_type":"text"},"source":["## Other useful resources\n","* [More GAN `keras` examples](https://github.com/eriklindernoren/Keras-GAN#cyclegan)\n","\n","* [GAN in medical imaging](https://github.com/xinario/awesome-gan-for-medical-imaging/blob/master/README.md#medical-image-synthesis)\n","\n","* [An example on filamentary structured images](https://web.bii.a-star.edu.sg/archive/machine_learning/Projects/filaStructObjs/Synthesis/index.html)\n","\n"]}]}