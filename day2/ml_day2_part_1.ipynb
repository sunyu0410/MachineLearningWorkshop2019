{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ml_day2_part_1.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"UFV_Rld6mOMt","colab_type":"text"},"source":["# Machine learning (60 minutes)\n","ACPSEM Machine Learning Workshop 2019, 29 - 31 May 2019\n","\n","Yu Sun, yu.sun@sydney.edu.au\n","\n","University of Sydney\n","\n","This session demonstrates several key concepts in machine learning:\n","* Correlation\n","* Data partitioning\n","* Model fitting\n","* Performance evaluation (including sensititvy and specificity)"]},{"cell_type":"markdown","metadata":{"id":"BgW7-BSwmnzf","colab_type":"text"},"source":["## Correlation (15 minutes)\n","The part shows how to compute the correlation coefficient of two vectors. First, we generate two *random* vectors."]},{"cell_type":"code","metadata":{"id":"65hCVeCfly34","colab_type":"code","colab":{}},"source":["# Generate two random vectors\n","import numpy as np\n","np.random.seed(1234)           # For reproducibility purposes\n","x1 = np.random.randn(1000)\n","x2 = np.random.randn(1000)\n","print('x1 first 50 items:', x1[:50])\n","print('x2 first 50 items:', x2[:50])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jb2v4egYnF4I","colab_type":"code","colab":{}},"source":["# Visualise\n","import matplotlib.pyplot as plt\n","plt.scatter(x=x1, y=x2, alpha=0.5)\n","plt.xlabel('x1')\n","plt.ylabel('x2')\n","plt.title('Scatter plot of x1 and x2')\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"sUNIKeSGnOIW","colab_type":"code","colab":{}},"source":["# Compute the Pearson correlation\n","import scipy.stats\n","print('Pearson correlation: %.3f, likelihood of non-correlated systems: %.3f' \\\n","      % scipy.stats.pearsonr(x1, x2))\n","print('Spearman correlation: %.3f, likelihood of non-correlated systems: %.3f' \\\n","      % scipy.stats.spearmanr(x1, x2))\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TQfTcYfgn7Sv","colab_type":"text"},"source":["Recall that these two vectors are independent, so that the correlation is almost zero. What about some correlated data?"]},{"cell_type":"code","metadata":{"id":"G4CezaCLoEFE","colab_type":"code","colab":{}},"source":["# Gnerate some non-independent data\n","np.random.seed(0)\n","x3 = np.random.randn(1000)\n","noise = np.random.randn(1000)\n","x4 = np.exp(x3) + noise"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JyB0fWutoe-K","colab_type":"code","colab":{}},"source":["# Visualise\n","import matplotlib.pyplot as plt\n","plt.scatter(x=x3, y=x4, alpha=0.5)\n","plt.xlabel('x3')\n","plt.ylabel('x4')\n","plt.title('Scatter plot of x3 and x4')\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XF8XPLsfovGA","colab_type":"text"},"source":["Compute the Pearson and Spearman correlation coefficients for `x3` and `x4`."]},{"cell_type":"code","metadata":{"id":"q7pMynJWo1wz","colab_type":"code","colab":{}},"source":["# You code goes here:\n","\n","\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"M4abbSJao1BL","colab_type":"text"},"source":["## Classifier training (45 minutes)\n","In this part, we will create a learning task and go through the key steps."]},{"cell_type":"markdown","metadata":{"id":"O6J4nYTZpIea","colab_type":"text"},"source":["### Geneate data for classification (5 minutes)"]},{"cell_type":"code","metadata":{"id":"6gl8C_2RpSrn","colab_type":"code","colab":{}},"source":["# Import the scikit-learn library\n","import sklearn\n","from sklearn.datasets import make_classification\n","from sklearn.preprocessing import StandardScaler\n","from matplotlib.colors import ListedColormap\n","from sklearn.model_selection import train_test_split"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"FcVMooCnpU8_","colab_type":"code","colab":{}},"source":["# Generate some dummy data\n","X, y = make_classification(n_samples=300, n_features=2, n_redundant=0, \n","                           n_informative=2, random_state=10, \n","                           n_clusters_per_class=1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"a2rYqwmkp_kR","colab_type":"code","colab":{}},"source":["# Visualise the data\n","# First we define a function to do the scatter plot\n","def plotData(x, y, title='Title'):\n","  'Scatter plot of the x, colour indicated by y'\n","  import matplotlib.pyplot as plt\n","  import matplotlib.patches as mpatches\n","  plt.style.use('ggplot')\n","  plt.scatter(x[:,0], x[:, 1], \n","              c=y, \n","              cmap=ListedColormap(['orange', 'darkcyan']), # for 0 and 1\n","              edgecolors='k',\n","              s=80)\n","  plt.xlabel('x1')\n","  plt.ylabel('x2')\n","  plt.title(title)\n","  patches = (mpatches.Patch(color='orange', label='Negative'),\n","             mpatches.Patch(color='darkcyan', label='Positive'))\n","  plt.legend(handles=patches)\n","  plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ihkbAHEQqBBx","colab_type":"code","colab":{}},"source":["plotData(X, y, 'Generated data for classification')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tV2I1CSxqnas","colab_type":"text"},"source":["### Data partitioning  (5 minutes)"]},{"cell_type":"code","metadata":{"id":"V-jeeg7LqqVB","colab_type":"code","colab":{}},"source":["# Split the data into training and test\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wFVliSymqtc8","colab_type":"code","colab":{}},"source":["# Check the dimensions\n","X_train.shape, X_test.shape, y_train.shape, y_test.shape"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Xa1pCv2Dzm_n","colab_type":"text"},"source":["Exercise: plot the training data and test data."]},{"cell_type":"markdown","metadata":{"id":"vVUQlmAcrC7n","colab_type":"text"},"source":["### Model fitting  (5 minutes)"]},{"cell_type":"code","metadata":{"id":"hEopQSqvrGwI","colab_type":"code","colab":{}},"source":["# Train a support vector machine (SVM) model\n","from sklearn.svm import SVC\n","svm_model = SVC(gamma=1, C=1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Lpak_yv7rJOr","colab_type":"code","colab":{}},"source":["# Train the model\n","svm_model.fit(X_train, y_train)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"P1I-jrRtrLQE","colab_type":"text"},"source":["### Performance evalution  (30 minutes)"]},{"cell_type":"markdown","metadata":{"id":"aTcXcLIerWR1","colab_type":"text"},"source":["#### Apply the model on the test data"]},{"cell_type":"code","metadata":{"id":"dtqgc3w9rKv5","colab_type":"code","colab":{}},"source":["# Predict on the test data\n","svm_pred = svm_model.predict(X_test)\n","print('Prediction (SVM): ', svm_pred)\n","print('Ground truth: ', y_test)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0QCuRwdprf74","colab_type":"text"},"source":["We need to choose a proper metric to *quantitatively* measure the performance of the model. The most straightfoward metric is **accuracy**."]},{"cell_type":"markdown","metadata":{"id":"YYGV3qLyrdBv","colab_type":"text"},"source":["#### Accuracy\n","Defined as the percentage that the prediction agrees with the ground truth (`y_test`)."]},{"cell_type":"code","metadata":{"id":"Ri4VfwQXru0S","colab_type":"code","colab":{}},"source":["# Compute the accuracy for the SVM model\n","sum(svm_pred == y_test) / svm_pred.size"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"P5iBSdCGr9ZB","colab_type":"text"},"source":["Explore the different component in the previous expression for accuracy, for example: what does it mean for `svm_pred == y_test`? Use `print()` to inspect each component."]},{"cell_type":"code","metadata":{"id":"kZwIEf8jsfI1","colab_type":"code","colab":{}},"source":["# You code goes here:\n","# e.g.\n","print(svm_pred==y_test)\n","\n","\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7LzbGptKv9Mc","colab_type":"text"},"source":["Exercise: for a learning task of imbalanced data (say 90% positive, 10% negative). If a model simply predict everything as positive, what's the accuracy? In another word, is accuracy suitable for all learning scenarios?\n","\n","The answer to that leads to sensitivity and specificity analysis."]},{"cell_type":"markdown","metadata":{"id":"itU2GRUEsoJD","colab_type":"text"},"source":["#### Sensitivity and Specificity\n","Write a function `calSS()` to calculate sensitivity and specificity given their definitions: $$Sensitivity = TP / (TP+FN)$$ $$Specificity = TN / (TN+FP)$$\n","\n","$TP$: true positive;\n","$TN$: true negative;\n","$FP$: false positive;\n","$FN$: false negative."]},{"cell_type":"code","metadata":{"id":"a2MRFO-xsnrL","colab_type":"code","colab":{}},"source":["def calSS(pred, truth):\n","    '''\n","    Compute the sensitivity and specificity of a prediction given the ground truth\n","    Input:\n","        pred - predicted outcome (binary)\n","        truth - ground truth (binary)\n","    Return: \n","        A tuple of sensitivity and specificity value\n","    '''\n","    tp = np.logical_and(pred==1, truth==1) # True Positivee (TP)\n","    fp = np.logical_and(pred==1, truth==0) # False Positive (FP)\n","    tn = np.logical_and(pred==0, truth==0) # True Negative (TN)\n","    fn = np.logical_and(pred==0, truth==1) # False Negative (FN)\n","\n","    sen = tp.sum() / (tp.sum() + fn.sum())\n","    spe = tn.sum() / (tn.sum() + fp.sum())\n","    return sen, spe"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"U19s06fbtG4E","colab_type":"code","colab":{}},"source":["# Calculate the results on our previous model\n","print(\"Sensitivity: %.2f, specificity: %.2f\" % calSS(svm_pred, y_test))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Li5wk87xvy-U","colab_type":"text"},"source":["Back to the last question, in an imbalanced dataset (90% positive, 10% negative), if a model simply output everything as positive, it will have a high sensitivity. However, it won't have a high specificity. \n","\n","*In another word, sensitivity and specifity give you a 'stereo' overview of the model.*\n","\n","We will use (sen, spe) as a short hand notation for a sensitivity and specificity pair."]},{"cell_type":"markdown","metadata":{"id":"kAGJTI2-uSq2","colab_type":"text"},"source":["#### Receiver operating characteristics (ROC) analysis"]},{"cell_type":"markdown","metadata":{"id":"zkR-cJrBtYPi","colab_type":"text"},"source":["Some models (e.g. the logistic regression) output a probabilistic output rather than a binary output. Now we train a logistic regression using the same training data."]},{"cell_type":"code","metadata":{"id":"mR70XsZPtXs6","colab_type":"code","colab":{}},"source":["# Initiate the logistic regression model\n","lor_model = sklearn.linear_model.LogisticRegression(solver='lbfgs')\n","# Train the model\n","lor_model.fit(X_train, y_train)\n","# Test the model\n","lor_pred = lor_model.predict_proba(X_test)\n","print(lor_pred)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CSg7tTuQtk5M","colab_type":"text"},"source":["For each row in the output, the two numbers give the probability that each test sample falls into the two categories. Notice, they sum up to 1.0 for each row.\n","\n","An issue arises as we cannot directly compute the (sen, spe) now. But what we can do is the first threshold the value, i.e. turn each row to a binary decision, then compute the (sen, spe). \n","\n","We will define a function `thres()` to do that."]},{"cell_type":"code","metadata":{"id":"UE-lbSs6xQvR","colab_type":"code","colab":{}},"source":["def thres(pred, t):\n","    '''\n","    Convert a continuous output to binary output using a threshold.\n","    Input:\n","        pred - the prediction vector\n","        t - a threshold\n","    Output:\n","        A binary vector\n","    '''\n","    return (pred >= t) * 1"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"xlb52Vz1xZ1u","colab_type":"code","colab":{}},"source":["# Test it on the logistic regression output using a threshold of 0.5\n","lor_pred_binary = thres(lor_pred[:, 1], t=0.5)\n","print(lor_pred_binary)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"16vHH-VvxjFP","colab_type":"code","colab":{}},"source":["# Now we can compute the (sen, spe)\n","print(\"Sensitivity: %.2f, specificity: %.2f\" % calSS(lor_pred_binary, y_test))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"csgKHbisxd6a","colab_type":"text"},"source":["The result above is dependent on the *threshold*. For a given threshold, we can convert the continuous output into binary. Then we can compute the sensitivity and specificity.\n","\n","What if we create **a series of thresholds** and compute **the corresponding (sen, spe)** respectively?"]},{"cell_type":"code","metadata":{"id":"290i13BNyGJI","colab_type":"code","colab":{}},"source":["# Create a series of thresholds\n","ts = range(0, 11)\n","\n","# Create variables to hold the (sen, spe) pairs\n","lor_pred_sen = []\n","lor_pred_spe = []\n","\n","# Iterate through each threshold and compute the corresponding (sen, spe) pairs\n","for eachT in ts:\n","    sen, spe = calSS(thres(lor_pred[:, 1], t=eachT/10.0), y_test)\n","    lor_pred_sen.append(sen)\n","    lor_pred_spe.append(spe)\n","    \n","# Print the result\n","for i in range(0, len(ts)):\n","    print('Thresholded at %s, sensitivity: %.3f, specificity: %.3f' % \n","          (i/10.0, lor_pred_sen[i], lor_pred_spe[i]))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2MG_fsuNyNBc","colab_type":"text"},"source":["Plot the (sen, spe) points, what do we get? This will give the ROC curve."]},{"cell_type":"code","metadata":{"id":"so3pnHHxyUQm","colab_type":"code","colab":{}},"source":["# Plot the (sen, spe) pairs\n","import numpy as np\n","plt.step(x=np.array(lor_pred_spe), y=np.array(lor_pred_sen), where='post')\n","plt.xlabel('Specificity')\n","plt.ylabel('Sensitivity')\n","plt.title('Receiver Operating Charcteristics Curve')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QR0axkHoyiZ4","colab_type":"text"},"source":["By convention, the x-axis is '1-specificity'."]},{"cell_type":"code","metadata":{"id":"lv2TZxl6yrf7","colab_type":"code","colab":{}},"source":["# Change the x to (1-spe) for the ROC curve\n","plt.step(x=1-np.array(lor_pred_spe), y=np.array(lor_pred_sen), where='post')\n","plt.xlabel('1-Specificity')\n","plt.ylabel('Sensitivity')\n","plt.title('Receiver Operating Charcteristics Curve')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"viLbw4Jky3b2","colab_type":"text"},"source":["The area under the ROC curve (AUC of ROC) is a measure of model performance.\n","\n","In summary, in terms of robustness:\n","$$ROC >  (sen, spe) > accuracy$$\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"k9hWBzQFyzRX","colab_type":"code","colab":{}},"source":["# Print the session information for reproducibility purposes\n","import IPython\n","print(IPython.sys_info())"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Xfxtb2JEzZcb","colab_type":"text"},"source":["\n","\n","---\n","\n","This is the end of the session."]}]}