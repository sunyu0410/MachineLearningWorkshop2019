{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ml_day2_part_4.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"D86ES-4Hxqrp","colab_type":"text"},"source":["# Unsupervised learning (30 minutes)\n","ACPSEM Machine Learning Workshop 2019, 29 - 31 May 2019\n","\n","Yu Sun, yu.sun@sydney.edu.au\n","\n","University of Sydney\n","\n","In this section, we will have a look at unsupervised learning using **principal component analysis** as an example. \n","\n","**Exercise** - Unsupervised learning is typically used when the ground truth label is not available. Now, spend three minutes and name some data at your hand that could be used for a unsupervised project. This could be a survival prediction, a segmentation or an image registration task. Discuss with your neighbours.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"KPXA6Ekq4QLn","colab_type":"text"},"source":["## What does PCA do?\n","In a nutshell, PCA constructs a new set of dimensions (i.e. principal components, or PCs) with the following properties:\n","* the principal coponents are orthogonal, which means they are perpendicular to each other, hence no correlation between PCs;\n","* the information contained in each PC is in a descending order. E.g. The first PC contains the highest amount of information among all PCs, and the last PC contains the least information. \n"]},{"cell_type":"markdown","metadata":{"id":"1ieMFzGgc8lO","colab_type":"text"},"source":["## Why these properties are favourable? \n","Recall that in machine learning, we typicall face two common challenges:\n","* the features can be highly correlated, which will lead to instable models;\n","* due to the optimal sample-to-feature ratio, only a subset of features should be used.\n","\n","The resultant properties from PCA provide a way to deal with these two issues. "]},{"cell_type":"markdown","metadata":{"id":"7Z66cbWSdEgJ","colab_type":"text"},"source":["Let's move on and look at a simple example."]},{"cell_type":"markdown","metadata":{"id":"5Ig98Bi4__B_","colab_type":"text"},"source":["## The first example\n"]},{"cell_type":"markdown","metadata":{"id":"m4_2i1gsovV9","colab_type":"text"},"source":["### Generate and visualise data\n","First, we will generate a dataset with two features, `x1` and `x2`. There will be some correlation between these two features."]},{"cell_type":"code","metadata":{"id":"KUFj7WKE4Q2V","colab_type":"code","colab":{}},"source":["# Import numpy\n","import numpy as np\n","\n","# Generate x1 and x2\n","mean = [0, 0]\n","cov = [[10, 5], [5, 10]] # Correlation is specified in this covariance matrix\n","np.random.seed(1234)\n","x1, x2 = np.random.multivariate_normal(mean, cov, 500).T\n","\n","# Examine the Pearson correlation\n","print(\"The correlation matrix is\\n\", np.corrcoef(x1, x2))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hu8mEjecCQVU","colab_type":"text"},"source":["Let's define a function `plotScatter()` and visualise the data we just created."]},{"cell_type":"code","metadata":{"id":"ALxGORoX4Ppj","colab_type":"code","colab":{}},"source":["# Define a function to plot the data\n","import matplotlib.pyplot as plt\n","plt.style.use('ggplot')\n","\n","def plotScatter(x1, x2, title=\"Scatter plot\", alpha=0.5):\n","  plt.scatter(x=x1, y=x2, alpha=alpha)\n","  plt.xlabel('x1')\n","  plt.ylabel('x2')\n","  plt.title(title)\n","  plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BjTOfl_rBvGq","colab_type":"code","colab":{}},"source":["# Visualise\n","plotScatter(x1, x2)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ORqXjInRCa8A","colab_type":"text"},"source":["We can see the data mainly lies in the 45 degree diagonal direction, which will corresponds to the *first principal components*. This is the direction, if we project the data onto it, contains the **largest** variance of the data.\n"]},{"cell_type":"markdown","metadata":{"id":"o0qaJzyio2kN","colab_type":"text"},"source":["### Perform PCA\n","Now we will perform PCA to extract the first and second principal components."]},{"cell_type":"code","metadata":{"id":"okzPBjg8yGqa","colab_type":"code","colab":{}},"source":["# Import the required functions\n","from sklearn.decomposition import PCA\n","from sklearn.preprocessing import StandardScaler\n","\n","# Prepare the data\n","data = np.array((x1, x2)).T\n","\n","# Normalise the data to mean 0, standar deviation 1\n","scaler = StandardScaler()\n","data_scaled = scaler.fit(data).transform(data)\n","\n","# Perform PCA\n","pca = PCA()\n","pca.fit(data_scaled)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sRcPzDwto8EB","colab_type":"text"},"source":["### Inspect results\n","The result of PCA is stored in the `pca` object itself. Let's see what's in there.\n"]},{"cell_type":"code","metadata":{"id":"eScBBJikH2KL","colab_type":"code","colab":{}},"source":["print(\"Number of PCs:\", pca.n_components_)\n","print(\"First PC:\", pca.components_[0, :]) # Python index starts with 0\n","print(\"Second PC:\", pca.components_[1, :])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9Tbsq-nWl28G","colab_type":"text"},"source":["We can see that \n","* the first PC is defined by the vector of (-0.707, -0.707), which is the 45 degree diagonal line. This agrees with the plot. \n","* the second PC is defined by the vector of (-0.707, 0.707).\n","\n","PCs are simply a new set of axes (or directions), which is a linear combination of the orignal axes. Based on the result:\n","\n","$$PC1 = (-0.707)*x_1 + (-0.707)*x_2$$\n","$$PC2 = (-0.707)*x_1 + (+0.707)*x_2$$\n","\n","In this case, **there are only two PCs, as the original data only contains two dimensions**. If the original data has 100 features, the PCA will result in 100 PCs. However, some of them may contain very little information and can be excluded. This is the basis for feature selection.\n"]},{"cell_type":"markdown","metadata":{"id":"r1qm1vFvpE_e","colab_type":"text"},"source":["### Examine properties\n","\n","Let's examine the properties (orthogonality and variance ranking) of the results."]},{"cell_type":"code","metadata":{"id":"AluJAPj6IWDE","colab_type":"code","colab":{}},"source":["# Orthogonality\n","print(\"The dot product (inner product) of two PCs is\", \n","      np.dot(pca.components_[:, 0], pca.components_[:, 1]))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DCk_65xMoFej","colab_type":"text"},"source":["Two perpendicular vectors will have an inner product of 0."]},{"cell_type":"code","metadata":{"id":"nhk9373bnRAH","colab_type":"code","colab":{}},"source":["# Ranking of variance\n","print(\"Variance first PC contains is %3.2f.\" % pca.explained_variance_[0])\n","print(\"Variance second PC contains is %3.2f.\" % pca.explained_variance_[1])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QSpYNjvnnrck","colab_type":"code","colab":{}},"source":["# Ranking of variance\n","print(\"Percentage variance first PC: %3.2f.\" % pca.explained_variance_ratio_[0])\n","print(\"Percentage variance second PC: %3.2f.\" % pca.explained_variance_ratio_[1])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_qjRk_NLoLpB","colab_type":"text"},"source":["We can see that first PC contains the dominant variance (78%) of all the data, and the second PC contains the remaining 22%."]},{"cell_type":"markdown","metadata":{"id":"1qI9JbjopZBn","colab_type":"text"},"source":["### Transform to PCs\n","Since PCs are a set of axes, we can project our data this set of axes. This is equivalent to keeping the data points in the plot, but draw another pair of axes. Hence, in the axes defined by the PCs, each data point will have a different coordinate."]},{"cell_type":"code","metadata":{"id":"1x0lVcxRnrXo","colab_type":"code","colab":{}},"source":["# Transform the data to PCs\n","data_scaled_pca = pca.transform(data_scaled)\n","print(\"The shape of the data prior to transformation:\", data_scaled.shape)\n","print(\"The shape of the data after transformation:\", data_scaled_pca.shape)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"MoixqgiZuF74","colab_type":"code","colab":{}},"source":["# Examine some coordinates\n","print(\"The first data prior to transformation:\", data_scaled[0, :])\n","print(\"The first data prior to transformation:\", data_scaled_pca[0, :])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DRn0trTJoRUg","colab_type":"text"},"source":["The transformed data has the identical shape as the original data, but with different coordinates (as the axes have changed). "]},{"cell_type":"markdown","metadata":{"id":"SLcnnZonu9QZ","colab_type":"text"},"source":["We can also examined the correlation after transformation."]},{"cell_type":"code","metadata":{"id":"Zvqo4526wHbR","colab_type":"code","colab":{}},"source":["# Examine the Pearson correlation\n","# Notice computers may have truncation errors,\n","#  and zero is represented as a very small number (typically less than 1e-7)\n","print(\"The correlation matrix is\\n\", np.corrcoef(data_scaled_pca.T))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"v4E7SJI-wlZn","colab_type":"code","colab":{}},"source":["# Visualise the transformed data\n","plotScatter(data_scaled_pca[:,0], data_scaled_pca[:, 1])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_OdbaCauwyAA","colab_type":"text"},"source":["Now the correlation between `x1` and `x2` after transformed to the principal components is 0. No linear relationship can be seen on the scatter plot."]},{"cell_type":"markdown","metadata":{"id":"ScvkR7VPIU_z","colab_type":"text"},"source":["This is the end of the first example."]},{"cell_type":"markdown","metadata":{"id":"M_a0odDu0sCX","colab_type":"text"},"source":["## PCA for dimension reduction\n","Dimension reduction can be considered as a *special form* of feature selection. This is because the resultant features from PCA is a **linear combination** of the original features. Each new feature is a mix of one or more original features with corresponding weights. This is different from a typical feature selection (e.g. Random Forest) where a subset of the original atomic features. Hence PCA is often referred as a dimension reduction method. Nonetheless, the main idea is to reduce the number of feature used, which is the key to avoid over-fitting.\n","\n","We will look at an example using PCA for dimension reduction."]},{"cell_type":"markdown","metadata":{"id":"p3tp6s_z33B9","colab_type":"text"},"source":["### Inspecting the data\n","In this exercise, we will use T2w MRI images of a prostate and the texture features computed from it. It's a good use case for PCA as features engineered from the same source tend to contain similar information (hence they can be quite correlated). Using PCA we will extract the uncorrelated key information from the data.\n","\n","Upload the zipfile `day2_pca.zip` to the cloud using the left-hand-side panel. Unzip the file. Note that the exclamation mark at the beginning indicates that it's a shell command rather than a Python command."]},{"cell_type":"code","metadata":{"id":"aIbGnhAW3u23","colab_type":"code","colab":{}},"source":["! unzip day2_pca.zip"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9mhkJBZV4inA","colab_type":"text"},"source":["There are 47 images in total, one for the T2w image, one contour binary mask, 45 texture features (GLCM: 21, GLRLM:11, GLSZM: 11, LBP:2).  These images are inherently in the same image coordinate so no image registration is required.\n","\n","Now let's read the images into an object called `images`."]},{"cell_type":"code","metadata":{"id":"G1rHJunx4oEi","colab_type":"code","colab":{}},"source":["# Import the required library\n","import os\n","import skimage\n","import numpy as np\n","\n","# Specify the folder\n","DATA_DIR = 'images'\n","\n","# List all files\n","filelist = os.listdir(DATA_DIR)\n","filelist = [i for i in filelist if i!='contour.tiff']\n","filelist.sort()\n","\n","# Read all files (except the contour) into an object using list comprehension\n","images = np.array([skimage.io.imread(os.path.join(DATA_DIR, i)) \n","                            for i in filelist])\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XyJr1jLt5c_8","colab_type":"text"},"source":["Let's have a look at the first image."]},{"cell_type":"code","metadata":{"id":"0aAVBKLd7Oht","colab_type":"code","colab":{}},"source":["# Inspect one of the image\n","import matplotlib.pyplot as plt\n","import matplotlib.cm as cm\n","plt.style.use('default')\n","plt.imshow(images[10], cmap=cm.gray)\n","plt.title(filelist[10])\n","plt.colorbar()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EFcpk5Ay5hCx","colab_type":"text"},"source":["What abou the prostate contour?"]},{"cell_type":"code","metadata":{"id":"Dg5zJHND8Bpx","colab_type":"code","colab":{}},"source":["# Read and show the contour\n","contour = skimage.io.imread('images/contour.tiff')\n","plt.imshow(contour, cmap=cm.gray)\n","plt.colorbar()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"dXXBcxRh_T0N","colab_type":"code","colab":{}},"source":["# Print out the unique values\n","# Useful for label maps\n","print(\"The contour contains these unique values: %s\" % np.unique(contour))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"m8lR7UCk5vvW","colab_type":"text"},"source":["### Preparing the data\n","We will use `pandas` to organise the data. The data frame data structure from `pandas` provides a convenient interface to interact with the data."]},{"cell_type":"code","metadata":{"id":"XUIf3Igq5vnC","colab_type":"code","colab":{}},"source":["# Import pands\n","import pandas as pd\n","\n","# Specify the column names\n","# It's simply the filenames with extensions\n","column_names = [os.path.splitext(i)[0] for i in filelist \n","                            if i != 'contour.tiff']\n","\n","# Create the data frame\n","# We pull all images into one long array using the reshape method\n","df = pd.DataFrame(images.reshape((images.shape[0], \n","                                  images.shape[1]*images.shape[2])).T, \n","                  columns=column_names)\n","\n","# Print out the dimension of the data frame\n","print(\"The data frame has %d rows and %d columes.\" % df.shape)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YNu_sBZA6ll3","colab_type":"text"},"source":["### Region of interest\n","**Exercise**: Recall from the simple example earlier, we have 500 samples and 2 dimensions. When we switch to imaging data, what are the samples and the dimensions? What are their numbers?\n","\n","Before we perform PCA on the data, we should isolate the voxels within the prostate region. (Take a minute and think what difference it would be if all voxels in the image are included.) To do that we use the contour to *mask* all the other images:"]},{"cell_type":"code","metadata":{"id":"yGr_Rcio9gSP","colab_type":"code","colab":{}},"source":["# Extract rows within the prostate contour\n","df_contour = df.iloc[np.where(contour.ravel()!=0)[0], ]\n","\n","# Print out the number of voxels within the prostate\n","print(\"There are %d pixels within the prostate contour.\" % df_contour.shape[0])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8pwN5UmX-NS0","colab_type":"text"},"source":["### Examine correlation\n","We can calculate and visualise the inter-feature correlation."]},{"cell_type":"code","metadata":{"id":"0d-MU9ZV_TJ8","colab_type":"code","colab":{}},"source":["# Calculate the correlation\n","corr = df_contour.corr(method='pearson')\n","\n","# Visualise correlation\n","import seaborn as sns\n","\n","def showCorrMatrix(corr):\n","  'Plot the correlation matrix.'\n","  mask = np.zeros_like(corr, dtype=np.bool)\n","  mask[np.triu_indices_from(mask)] = True\n","  fig=plt.figure(figsize=(18, 16), dpi= 80, facecolor='w', edgecolor='k')\n","  return sns.heatmap(corr, mask=mask, \n","                    cmap=sns.diverging_palette(220, 10, as_cmap=True), \n","                    vmax=0.3,center=0, square=True, \n","                    linewidths=0.4, cbar_kws={'shrink': 0.5})\n","\n","showCorrMatrix(corr)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BCm9Q1Cx-YTf","colab_type":"text"},"source":["### Performing PCA on the imaging data\n","Now it's time to do the PCA."]},{"cell_type":"code","metadata":{"id":"f6MAe7VdBBJ1","colab_type":"code","colab":{}},"source":["# Import the require functions\n","from sklearn.decomposition import PCA\n","from sklearn.preprocessing import StandardScaler\n","\n","# Normalise data\n","df_contour_value = df_contour.values\n","scaler2 = StandardScaler()\n","df_contour_norm = pd.DataFrame(scaler2.fit(df_contour).transform(df_contour),\n","                              columns=df_contour.columns)\n","\n","# Perform PCA\n","pca2 = PCA()\n","pca2.fit(df_contour_norm)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_Do9fjkUB9jM","colab_type":"text"},"source":["Similarly, we can get the variance and the percentage variance for each principal component."]},{"cell_type":"code","metadata":{"id":"skx6dCooBMHh","colab_type":"code","colab":{}},"source":["# Variance\n","print('Variance explained:\\n', pca2.explained_variance_)\n","print() # an extra line\n","\n","# Percentage variance\n","print('Percentage variance explained:\\n', pca2.explained_variance_ratio_)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GkiojbyQCZJ7","colab_type":"text"},"source":["It's much easier to visualise this in a plot than reading the numbers."]},{"cell_type":"code","metadata":{"id":"xAvkpmDUDaxX","colab_type":"code","colab":{}},"source":["# Visualise the variance for each PC\n","pc = ['PC ' + str(i+1).zfill(2) for i in range(df_contour.shape[1])]\n","variances = pca2.explained_variance_ratio_\n","\n","x_pos = [i for i, _ in enumerate(pc)]\n","plt.bar(x_pos, variances, color='darkcyan')\n","plt.xlabel(\"Principal Components\")\n","plt.ylabel(\"Variance (%)\")\n","plt.title(\"Percentage variance explained\")\n","plt.xticks(x_pos, pc, rotation='vertical', fontsize=6)\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XbTsLBQ8C7Z-","colab_type":"text"},"source":["When using PCA for dimension reduction, we need to pick a number $N$. The rule of thumb is to **select the first $N$ principal components so that 95% of the variance is retained**.\n","\n","It's hard from the previous plot to see at which PC the accumulative variance exceeds 95%. But we can simply plot out the accumulative varaince."]},{"cell_type":"code","metadata":{"id":"DOxdHaacF_TD","colab_type":"code","colab":{}},"source":["# Plot the data\n","pc = ['PC ' + str(i+1).zfill(2) for i in range(df_contour.shape[1])]\n","variance_acc = np.add.accumulate(pca2.explained_variance_ratio_)\n","\n","plt.plot(variance_acc, '--o', markersize=5, color='darkcyan')\n","plt.xlabel(\"Principal Components\")\n","plt.ylabel(\"Accumulative Variance (%)\")\n","plt.title(\"Accumulative variance explained\")\n","plt.axhline(y=0.95, color='gold', linestyle='--')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2Hgy12RfDv6P","colab_type":"text"},"source":["We can see that the first 9 PCs will have a total percentage varaince over 95%. In practise, we will choose 9 in this case. This is how we use PCA for dimension reduction.\n","\n","But wait, what exactly are these 9 PCs?\n","\n","**Exercise**: How do we get each PC? Refer to the first example and get PC1 in this case."]},{"cell_type":"markdown","metadata":{"id":"a4tcxfJPI9lD","colab_type":"text"},"source":["### Examine the relationship between features\n","The coefficient for each PC contain the relationship of the features. For example, we know PC1 contains the highest variance. By examining its composition, we can tell how the original features are combined to form PC1.\n","\n","The following code prints out the top 10 most important features that make up PC1. "]},{"cell_type":"code","metadata":{"colab_type":"code","id":"_QuuO8xhV7Ln","colab":{}},"source":["# Extract the coefficient of PC1\n","pc1_coeff = pca2.components_[0, :]\n","\n","# Construct a data frame for PC1\n","pc1 = pd.DataFrame(pc1_coeff, index=df_contour_norm.columns, \n","                   columns=['coefficient'])\n","print(pc1.sort_values('coefficient')[:10])\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rcKexykHTqy7","colab_type":"text"},"source":["This shows that when the original features are linearly combined in this manner the information contained is maximised. The coefficients indicate the 'effective proportion' or the association between features. This reveals the intrinsic pattern within the data, without any use of labels (the definition of unsupervised learning).\n","\n","We can visualise the data using bar plots. Here it's the relative magnitude matters."]},{"cell_type":"code","metadata":{"id":"pkkO6GwgYRDA","colab_type":"code","colab":{}},"source":["# Visualise the composition of PC1\n","x_pos = [i for i, _ in enumerate(pc1.index)]\n","plt.bar(x_pos, pc1.coefficient, color='darkcyan')\n","plt.xlabel(\"Original features\")\n","plt.ylabel(\"Coefficient (%)\")\n","plt.title(\"Composition of PC1\")\n","plt.xticks(x_pos, pc1.index, rotation='vertical', fontsize=6)\n","plt.ylim((-0.25, 0.25))\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0aQ-97OSEet0","colab_type":"text"},"source":["### Examine the correlations\n","We can examine the correlation for the PCs, which should be zero theoretically."]},{"cell_type":"code","metadata":{"id":"sfC-LVr7Evl2","colab_type":"code","colab":{}},"source":["# Get the PCs\n","df_contour_norm_pca = pd.DataFrame(pca2.transform(df_contour_norm),\n","          columns=['PC'+str(i+1) for i in range(df_contour_norm.shape[1])])\n","\n","# Plot the correlation matrix\n","showCorrMatrix(df_contour_norm_pca.corr())"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"x3aTma6QFoJn","colab_type":"text"},"source":["## Wrap up\n","PCA is a widely used unsupervised algorithm for unsupervised learning. It decomposes the original dimensions in the data and reconstructs a new set of dimensions, known as principal components. Two important properties of PCs:\n","* they are orthogonal (zero correlation); and\n","* the order corresponds to the amount of information contained (measured by variance).\n","\n","You can apply PCA for dimension reduction by choosing the first $N$ principal components that retain 95% total variance.\n","\n","\n","However, PCA does come with some limitations. Here are some:\n","* it's sensitive to scaling, so normalisation is important;\n","* the resultant PC is a mix of the original features, hence interpretation is compromised. For example, what is the meaning of $0.5*T2w + 0.3*ADC$?\n","* if your model takes PC as features, the *exact* preprocessing procedure should be applied on the test data in order to use the model.\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"3xUcxEyFxR_2","colab_type":"code","colab":{}},"source":["# Print the session information for reproducibility purposes\n","import IPython\n","print(IPython.sys_info())"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"u6WHvjgcxThk","colab_type":"text"},"source":["\n","\n","---\n","\n","This is the end of the session.\n"]}]}