{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ml_day2_part_3.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"NbNnjmhyKMMf","colab_type":"text"},"source":["# Over-fitting (20 minutes)\n","ACPSEM Machine Learning Workshop 2019, 29 - 31 May 2019\n","\n","Yu Sun, yu.sun@sydney.edu.au\n","\n","University of Sydney\n","\n","In this session, we will look at a case of over-fitting. This will emphasise the need for data partitioning. First let's generate some sample data for a regression task."]},{"cell_type":"code","metadata":{"id":"HmzRAFexK3LH","colab_type":"code","colab":{}},"source":["# Import libraries\n","import numpy as np\n","\n","# Generate some x values\n","np.random.seed(1234)\n","x = np.random.randn(30)\n","\n","# Define a function to generate the data\n","def genData(x):\n","  '''Returns the GROUND TRUTH y for the given x'''\n","  y = np.exp(0.5*x+1) # use an exponential function\n","  return y\n","\n","# Generate some data\n","y_true = genData(x)\n","\n","# The observed data (data with noise)\n","noise = np.random.randn(y_true.size)\n","y = y_true + noise"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Uy-rwIXXOvb9","colab_type":"text"},"source":["Here we use an exponential function (`np.exp()`) as the ground truth. Let's have a look at the data."]},{"cell_type":"code","metadata":{"id":"O4Pr8T8lL67x","colab_type":"code","colab":{}},"source":["# Define a function to plot the data\n","import matplotlib.pyplot as plt\n","\n","# Plot the observed data\n","plt.scatter(x=x, y=y, alpha=0.5, label='Observation')\n","\n","# Plot the ground truth data\n","x_grid = np.linspace(x.min(), x.max(), 100)\n","plt.plot(x_grid, genData(x_grid), linewidth=3, label='Ground truth')\n","\n","# Configure and show the plot\n","plt.xlabel('x')\n","plt.ylabel('y')\n","plt.title('Scatter plot of x, observed y and true y')\n","plt.legend(loc='best')\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"27w2MFFYSMJg","colab_type":"text"},"source":["Now let's fit the data with polynomials with incrementing degrees."]},{"cell_type":"code","metadata":{"id":"M7Vv5nqXSLm4","colab_type":"code","colab":{}},"source":["# Import the function\n","from sklearn.preprocessing import PolynomialFeatures\n","from sklearn.linear_model import LinearRegression\n","\n","# Define a function to fit the model\n","def polyExp(x, n):\n","  '''Fitting the data x using n-degree polynomials.'''\n","  # Get the polynomial expansions\n","  poly = PolynomialFeatures(degree=n)\n","  x_poly = poly.fit(x[:,np.newaxis]).transform(x[:,np.newaxis])\n","  return x_poly"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8indOzKteii0","colab_type":"text"},"source":["For a given $x$ and $n$, the polynomial expasion will be\n","\n","$$polyExp(x)=x^0, x^1, x^2, ..., x^n$$"]},{"cell_type":"code","metadata":{"id":"-SJ96KrHdfWw","colab_type":"code","colab":{}},"source":["# Test the polyExp()\n","x_poly = polyExp(x, 3)\n","print(\"The 3rd-degree polynomial expasion for\", x[0], \"is\", x_poly[0])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RqkThep5fBnx","colab_type":"text"},"source":["Now, we can do a polynomial fit at different degrees."]},{"cell_type":"code","metadata":{"id":"XlQ1SUsDfILt","colab_type":"code","colab":{}},"source":["# Define a function to do that fitting\n","def polyFit(x, y, n):\n","  '''Return the n-th order polynomial regression for (x, y)'''\n","  x_poly = polyExp(x, n)\n","  # Polynomial regression is simply linear regression including polynomial terms\n","  reg = LinearRegression()\n","  reg.fit(x_poly, y)\n","  return reg"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"skgpI5Udf8uc","colab_type":"text"},"source":["Now we can test `polyFit()` and include the 3rd-degree polynomial fit in the plot."]},{"cell_type":"code","metadata":{"id":"x4GI4yrJgSL2","colab_type":"code","colab":{}},"source":["# Get the fitted model\n","reg = polyFit(x, y, 3)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"tk2YWJMyf8QY","colab_type":"code","colab":{}},"source":["# Plot the observed data\n","plt.scatter(x=x, y=y, alpha=0.5, label='Observation')\n","\n","# Plot the ground truth data\n","x_grid = np.linspace(x.min(), x.max(), 100)\n","plt.plot(x_grid, genData(x_grid), linewidth=3, label='Ground truth')\n","\n","################## The extra lines of code #########################\n","# Plot the polynomial fit\n","plt.plot(x_grid, reg.predict(polyExp(x_grid,3)), \n","         label='3-degree polynomial')\n","####################################################################\n","\n","# Configure and show the plot\n","plt.xlabel('x')\n","plt.ylabel('y')\n","plt.title('Ground truth, obvserved data and polynomial fit(s)')\n","plt.legend(loc='best')\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gMdID81xiBPb","colab_type":"text"},"source":["Exercise: refer to how the 3rd-order polynomial regression, add to the plot the 1st, 2nd, 4th and 5th order polynomials. Compare your code with the followings."]},{"cell_type":"code","metadata":{"id":"9WbYTgU1iV2r","colab_type":"code","colab":{}},"source":["# Plot the observed data\n","plt.scatter(x=x, y=y, alpha=0.5, label='Observation')\n","\n","# Plot the ground truth data\n","x_grid = np.linspace(x.min(), x.max(), 100)\n","plt.plot(x_grid, genData(x_grid), linewidth=3, label='Ground truth')\n","\n","################## The extra lines of code #########################\n","# Plot the polynomial fits\n","for n in [1,2,3,4,5]:\n","  reg = polyFit(x, y, n)\n","  plt.plot(x_grid, reg.predict(polyExp(x_grid,n)), alpha=0.5,\n","              label=str(n) + '-degree polynomial')\n","####################################################################\n","\n","# Configure and show the plot\n","plt.xlabel('x')\n","plt.ylabel('y')\n","plt.title('Ground truth, obvserved data and polynomial fit(s)')\n","plt.legend(loc='best')\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"koiwuK52kl4A","colab_type":"text"},"source":["Exercise: do the same for up to 15. "]},{"cell_type":"code","metadata":{"id":"k1froN6EkldI","colab_type":"code","colab":{}},"source":["# 6 to 10\n","# Plot the observed data\n","plt.scatter(x=x, y=y, alpha=0.5, label='Observation')\n","# Plot the ground truth data\n","x_grid = np.linspace(x.min(), x.max(), 100)\n","plt.plot(x_grid, genData(x_grid), linewidth=3, label='Ground truth')\n","# Plot the polynomial fit\n","for n in [6,7,8,9,10]:\n","  reg = polyFit(x, y, n)\n","  plt.plot(x_grid, reg.predict(polyExp(x_grid,n)), alpha=0.5,\n","              label=str(n) + '-degree polynomial')\n","# Configure and show the plot\n","plt.xlabel('x')\n","plt.ylabel('y')\n","plt.legend(loc='lower left', bbox_to_anchor= (0.0, 1.01), \n","            borderaxespad=0, frameon=False)\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8wYRrTdjnS5X","colab_type":"code","colab":{}},"source":["# 11 to 15\n","# Plot the observed data\n","plt.scatter(x=x, y=y, alpha=0.5, label='Observation')\n","# Plot the ground truth data\n","x_grid = np.linspace(x.min(), x.max(), 100)\n","plt.plot(x_grid, genData(x_grid), linewidth=3, label='Ground truth')\n","# Plot the polynomial fit\n","for n in [11,12,13,14,15]:\n","  reg = polyFit(x, y, n)\n","  plt.plot(x_grid, reg.predict(polyExp(x_grid,n)), alpha=0.5,\n","              label=str(n) + '-degree polynomial')\n","# Configure and show the plot\n","plt.xlabel('x')\n","plt.ylabel('y')\n","plt.legend(loc='lower left', bbox_to_anchor= (0.0, 1.01), \n","            borderaxespad=0, frameon=False)\n","\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2uYJKe8wn9eX","colab_type":"text"},"source":["We can calculate the mean square error (MSE) agains the degree of polynomial:\n","\n","$$MSE=\\sqrt{\\frac{[y_{ob}-y_{pred}]^2}{N}}$$\n","\n","where $y_{ob}$ is the observed data (training data), $y_{pred}$ is the predicted value and $N$ is the number of samples."]},{"cell_type":"code","metadata":{"id":"rUkYBZSVo-yH","colab_type":"code","colab":{}},"source":["# Define a function to calculate MSE\n","def calMSE(y, y_pred):\n","  '''Calculate the MSE given y and predicted y.'''\n","  return np.sqrt(((y-y_pred)**2).mean()) # Power indicated by **"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"lYpIySk1qYWp","colab_type":"code","colab":{}},"source":["# Calculate the corresponding MSE\n","degrees = range(1, 16) # 1 to 15\n","mse_s = []\n","for n in degrees:\n","  y_pred = polyFit(x, y, n).predict(polyExp(x, n))\n","  mse = calMSE(y, y_pred)\n","  mse_s.append(mse)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Bu5zi5tMsqRP","colab_type":"code","colab":{}},"source":["# Plot the MSE against the polynomial degrees\n","plt.plot(degrees, mse_s, '-o')\n","plt.xlabel('Polynomial degrees')\n","plt.ylabel('Mean squared error')\n","plt.title('Mean squared error vs polynomial degrees')\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4niIzkyUtYph","colab_type":"text"},"source":["We can see that the MSE is always decreasing. However, at some point, the model lowers its MSE by fitting the non-important information in the data given, which is not generalisable to other data.\n","\n","Data partitioning is the easist way to detect over-fitting."]},{"cell_type":"code","metadata":{"id":"503X7U0Pt0sN","colab_type":"code","colab":{}},"source":["# Partition the data into training and test data\n","from sklearn.model_selection import train_test_split\n","x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, \n","                                                    random_state=0)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"N5P8NwmDufMl","colab_type":"text"},"source":["Now we do the previous plot again, but plot the training and test data seperately."]},{"cell_type":"code","metadata":{"id":"HvxpynLKupd1","colab_type":"code","colab":{}},"source":["# Calculate the training MSE\n","degrees = range(1, 11) # 1 to 10\n","mse_s_tr = [] # training MSE\n","mse_s_ts = [] # test MSE\n","for n in degrees:\n","  # Calculate training MSE\n","  y_pred_tr = polyFit(x_train, y_train, n).predict(polyExp(x_train, n))\n","  mse_tr = calMSE(y_train, y_pred_tr)\n","  mse_s_tr.append(mse_tr)\n","  # Calculate test MSE\n","  y_pred_ts = polyFit(x_train, y_train, n).predict(polyExp(x_test, n))\n","  mse_ts = calMSE(y_test, y_pred_ts)\n","  mse_s_ts.append(mse_ts)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"lpQkDaR4vbE1","colab_type":"code","colab":{}},"source":["# Plot the MSE \n","plt.plot(degrees, mse_s_tr, '-o', label='Training data')\n","plt.plot(degrees, mse_s_ts, '-o', label='Test data')\n","plt.legend(loc='best')\n","plt.xlabel('Polynomial degrees')\n","plt.ylabel('Mean squared error')\n","plt.title('Mean squared error vs polynomial degrees')\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UWUEc1CiwH3t","colab_type":"text"},"source":["Can you see at which point the overfitting starts? Why?"]},{"cell_type":"code","metadata":{"id":"tSi9SibsxMzy","colab_type":"code","colab":{}},"source":["# Print the session information for reproducibility purposes\n","import IPython\n","print(IPython.sys_info())"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iH0f7UMpxJP1","colab_type":"text"},"source":["\n","\n","---\n","\n","This is the end of the session."]}]}